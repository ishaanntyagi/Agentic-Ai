{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from pprint import pprint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key    = os.getenv(\"OPENAI_API_KEY\")\n",
    "gemini_api_key    = os.getenv(\"GEMINI_API_KEY\")\n",
    "groq_api_key      = os.getenv(\"GROQ_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b71af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key = gemini_api_key)\n",
    "llm2 = ChatGroq(model = \"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6f7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_messages = [HumanMessage(content=\"Hey Iam Ishn Do you rememeber me ?\"),\n",
    "            AIMessage(content=\"Hey ishn i do remember you\")]\n",
    "\n",
    "final_messages = [HumanMessage(content=\"Do you remember my name ? Also I wanna Learn how to implement a Trimmer in Messages and Chat context\")]\n",
    "\n",
    "add_messages = initial_messages + final_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d435411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2.invoke(add_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891a9a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user mentioned their name is Ishn and asked if I remember. I should confirm that I do remember their name. Then they want to learn how to implement a trimmer in messages and chat context. Let me break this down.\\n\\nFirst, \"trimmer\" in the context of messages and chat probably refers to trimming or shortening messages. Users might want to limit message length for UI purposes, performance, or to fit within system constraints. So, the user is looking for ways to implement such a feature.\\n\\nI should consider different scenarios where trimming is needed, like truncating long messages with an ellipsis, limiting input length, or summarizing content. Maybe they want to prevent messages from being too long in a chat app. I need to explain the steps involved in each approach.\\n\\nFor truncating display text, using JavaScript\\'s slice or substring methods, adding an ellipsis, and ensuring it\\'s responsive. For input length limits, setting max length attributes in HTML form elements and validating on the front-end and back-end. For summarization, maybe using algorithms or machine learning models to condense messages.\\n\\nAlso, the user might not have mentioned specific technologies, so I should provide general approaches applicable to different programming languages and frameworks. Mentioning libraries or tools could be helpful if they\\'re using specific tech stacks.\\n\\nWait, the user\\'s name is Ishn. Maybe they\\'re a developer working on a chat application, possibly a web app. They might be using JavaScript, but I shouldn\\'t assume. I should keep the answer flexible, covering different aspects like front-end and back-end implementations, client-side and server-side validation, and different methods of trimming.\\n\\nI should also highlight best practices, like handling edge cases (messages exactly at the limit, empty strings), maintaining user experience (not cutting words in the middle), and accessibility considerations when truncating text.\\n\\nLet me structure the answer step by step, starting from understanding the requirements, then moving to implementation examples in code, and finally discussing best practices. That way, Ishn can follow along and apply the methods to their specific project.\\n</think>\\n\\nHey, **Ishn**! I do remember your name. ðŸ˜Š  \\n\\nNow, about implementing a **trimmer** for messages and chat contextsâ€”letâ€™s break it down based on what you might need:\\n\\n---\\n\\n### **1. Trimming Display Text (UI/UX Optimization)**  \\nIf you want to **truncate long messages** (e.g., showing \"This is a very long...\" instead of the full text), hereâ€™s how to do it:\\n\\n#### **JavaScript Example**:\\n```javascript\\nfunction trimMessage(message, maxLength = 50) {\\n  if (message.length > maxLength) {\\n    return message.slice(0, maxLength) + \"...\";\\n  }\\n  return message;\\n}\\n\\n// Usage\\nconst longMessage = \"Hey Ishn, this is a super long message that needs trimming!\";\\nconsole.log(trimMessage(longMessage)); \\n// Output: \"Hey Ishn, this is a super long me...\"\\n```\\n\\n#### **CSS Alternative** (for visual truncation):\\n```css\\n.chat-message {\\n  white-space: nowrap;\\n  overflow: hidden;\\n  text-overflow: ellipsis;\\n  max-width: 200px; /* Adjust based on your layout */\\n}\\n```\\n\\n---\\n\\n### **2. Limiting Input Length (User Input Validation)**  \\nIf you want to **prevent users from sending overly long messages**, enforce a character limit:\\n\\n#### **HTML + JavaScript Example**:\\n```html\\n<textarea id=\"chat-input\" maxlength=\"200\"></textarea>\\n<p id=\"char-count\">200 characters remaining</p>\\n\\n<script>\\n  const input = document.getElementById(\"chat-input\");\\n  const count = document.getElementById(\"char-count\");\\n\\n  input.addEventListener(\"input\", () => {\\n    const remaining = 200 - input.value.length;\\n    count.textContent = `${remaining} characters remaining`;\\n  });\\n</script>\\n```\\n\\n#### **Backend Validation** (e.g., in Python/Flask):\\n```python\\nfrom flask import request\\n\\n@app.route(\"/send_message\", methods=[\"POST\"])\\ndef send_message():\\n    message = request.json.get(\"message\")\\n    if len(message) > 200:\\n        return {\"error\": \"Message too long!\"}, 400\\n    # Proceed to save/send message\\n```\\n\\n---\\n\\n### **3. Summarizing Long Messages (Advanced)**  \\nIf you want to **condense messages automatically** (e.g., for chat summaries), use a **text summarization model** (e.g., Hugging Face\\'s `transformers` library):\\n\\n#### **Python Example** (with Hugging Face):\\n```python\\nfrom transformers import pipeline\\n\\nsummarizer = pipeline(\"summarization\")\\n\\ndef summarize_message(text, max_length=50):\\n    summary = summarizer(text, max_length=max_length, min_length=25, do_sample=False)\\n    return summary[0][\"summary_text\"]\\n\\n# Usage\\nlong_text = \"This is a very long message that needs to be summarized for chat display.\"\\nprint(summarize_message(long_text))  # Output: \"A concise summary of the long message...\"\\n```\\n\\n---\\n\\n### **4. Edge Cases to Handle**  \\n- **Trimming mid-word**: Avoid cutting words by using `trim` with `lastIndexOf(\" \")` to find the last space before the limit.\\n- **Multiline Messages**: For line wrapping, use `line-clamp` in CSS (`-webkit-line-clamp`) or split on `\\\\n` in code.\\n- **Emojis/Unicode**: Ensure trimming respects Unicode characters (JavaScriptâ€™s `String.prototype.length` handles this).\\n\\n---\\n\\n### **When to Use Each Approach**  \\n| Scenario                     | Solution                          |\\n|------------------------------|-----------------------------------|\\n| Truncate display text        | CSS `text-overflow: ellipsis`     |\\n| Limit user input length      | HTML `maxlength` + JS validation |\\n| Summarize long content       | NLP summarization models          |\\n| Prevent performance issues   | Backend validation + caching      |\\n\\n---\\n\\nLet me know if you need help integrating this into a specific framework (React, Flutter, etc.) or handling emojis/Unicode edge cases! ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1270, 'prompt_tokens': 58, 'total_tokens': 1328, 'completion_time': 4.829873161, 'prompt_time': 0.022544859, 'queue_time': 0.044143301, 'total_time': 4.85241802}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9d98a39e-3ee7-4d36-9a31-5372c85c4fde-0', usage_metadata={'input_tokens': 58, 'output_tokens': 1270, 'total_tokens': 1328})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a2238",
   "metadata": {},
   "source": [
    "### ChatHistory Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c814819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc768cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44235b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_session_history = RunnableWithMessageHistory(llm, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7a91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hey Ishn!\\n\\nYes, you just told me your name is Ishn! I remember that from our current conversation. However, as an AI, I don\\'t have personal memory or a persistent identity for users across different conversations. So, I wouldn\\'t retain that information for future, separate chats unless you tell me again.\\n\\n---\\n\\nRegarding implementing a \"Trimmer\" in messages and chat context, that\\'s a great topic! The term \"trimmer\" can mean a few different things depending on your goal. Let\\'s break down the common interpretations and how you might implement them:\\n\\n1.  **Basic Whitespace Trimming:** Removing leading/trailing spaces, newlines, or tabs.\\n2.  **Message Truncation:** Shortening a message if it exceeds a certain character limit.\\n3.  **Content Filtering/Noise Reduction:** Removing specific types of irrelevant content (e.g., duplicate timestamps, extra metadata, specific keywords).\\n4.  **Redundancy Reduction/Summarization:** More advanced \"trimming\" to get the gist of a long message.\\n\\nLet\\'s look at how to implement each, primarily using Python as it\\'s common for text processing:\\n\\n---\\n\\n### 1. Basic Whitespace Trimming\\n\\nThis is the most fundamental kind of trimming, ensuring your messages are clean before processing or display.\\n\\n**Goal:** Remove spaces, tabs, and newlines from the beginning and end of a string.\\n\\n**Implementation (Python):**\\n\\n```python\\ndef trim_whitespace(message):\\n    return message.strip()\\n\\n# Examples\\nmessage1 = \"   Hello, world!   \"\\nmessage2 = \"\\\\n\\\\tThis is a test message.\\\\n\"\\nmessage3 = \"No extra spaces here.\"\\n\\nprint(f\"Original 1: \\'{message1}\\' -> Trimmed: \\'{trim_whitespace(message1)}\\'\")\\nprint(f\"Original 2: \\'{message2}\\' -> Trimmed: \\'{trim_whitespace(message2)}\\'\")\\nprint(f\"Original 3: \\'{message3}\\' -> Trimmed: \\'{trim_whitespace(message3)}\\'\")\\n```\\n\\n**Other Languages:**\\n*   **JavaScript:** `message.trim()`\\n*   **Java:** `message.trim()`\\n*   **C#:** `message.Trim()`\\n\\n---\\n\\n### 2. Message Truncation\\n\\nUseful for displaying messages in a limited space (e.g., chat previews, notifications) or for storing messages with a maximum length.\\n\\n**Goal:** Limit the message to a `max_length`, adding an ellipsis if it\\'s cut. It\\'s often good to cut at a word boundary if possible.\\n\\n**Implementation (Python):**\\n\\n```python\\ndef truncate_message(message, max_length=100, ellipsis=\"...\"):\\n    if len(message) <= max_length:\\n        return message\\n    \\n    # Calculate actual length available for text\\n    text_length = max_length - len(ellipsis)\\n    if text_length <= 0: # Handle very small max_length\\n        return ellipsis[:max_length] # Just return ellipsis if max_length is too small\\n\\n    truncated = message[:text_length]\\n    \\n    # Optional: Try to cut at a word boundary\\n    last_space = truncated.rfind(\\' \\')\\n    if last_space > 0:\\n        return truncated[:last_space] + ellipsis\\n    \\n    return truncated + ellipsis\\n\\n# Examples\\nlong_message = \"This is a very long message that needs to be truncated because it exceeds the maximum allowed length for display in a chat preview or notification area.\"\\nshort_message = \"This is a short message.\"\\n\\nprint(f\"Original Long: \\'{long_message}\\'\")\\nprint(f\"Truncated (50): \\'{truncate_message(long_message, 50)}\\'\")\\nprint(f\"Truncated (20): \\'{truncate_message(long_message, 20)}\\'\")\\nprint(f\"Truncated (10): \\'{truncate_message(long_message, 10)}\\'\")\\nprint(f\"Original Short: \\'{short_message}\\'\")\\nprint(f\"Truncated Short (50): \\'{truncate_message(short_message, 50)}\\'\")\\n```\\n\\n---\\n\\n### 3. Content Filtering / Noise Reduction\\n\\nThis involves removing specific patterns or types of information that might be considered \"noise\" or irrelevant in your chat context. This often uses regular expressions.\\n\\n**Goal:** Remove things like URLs, specific timestamps, or other structured data that isn\\'t central to the message\\'s meaning.\\n\\n**Implementation (Python):**\\n\\n```python\\nimport re\\n\\ndef remove_urls(message):\\n    # Regex for common URL patterns\\n    url_pattern = re.compile(r\\'https?://\\\\S+|www\\\\.\\\\S+\\')\\n    return url_pattern.sub(\\'\\', message).strip()\\n\\ndef remove_mentions(message):\\n    # Removes @username mentions\\n    mention_pattern = re.compile(r\\'@\\\\w+\\')\\n    return mention_pattern.sub(\\'\\', message).strip()\\n\\ndef remove_timestamps(message):\\n    # Example: Removes [HH:MM:SS] format timestamps\\n    timestamp_pattern = re.compile(r\\'\\\\[\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\]\\')\\n    return timestamp_pattern.sub(\\'\\', message).strip()\\n\\n# Example\\nchat_message = \"Hey @JohnDoe, check out this link: https://example.com/some/page [14:35:01]. It\\'s really cool! Also visit www.another.site\"\\n\\nprint(f\"Original: \\'{chat_message}\\'\")\\nmessage_no_urls = remove_urls(chat_message)\\nprint(f\"No URLs: \\'{message_no_urls}\\'\")\\nmessage_no_mentions = remove_mentions(chat_message)\\nprint(f\"No Mentions: \\'{message_no_mentions}\\'\")\\nmessage_cleaned = remove_timestamps(remove_mentions(remove_urls(chat_message)))\\nprint(f\"Fully Cleaned: \\'{message_cleaned}\\'\")\\n```\\n\\n---\\n\\n### 4. Redundancy Reduction / Summarization (More Advanced)\\n\\nThis is where \"trimming\" becomes more complex and often involves Natural Language Processing (NLP).\\n\\n**Goal:** Condense the core meaning of a very long message into a shorter form, removing redundant phrases or summarizing the main points.\\n\\n**Implementation (Python - requires libraries like NLTK, spaCy, or Hugging Face Transformers):**\\n\\n*   **Stop Word Removal:** Removing common words (like \"the\", \"is\", \"a\") that don\\'t add much meaning for some analyses.\\n*   **Text Simplification/Paraphrasing:** Rewriting sentences to be shorter or clearer.\\n*   **Extractive Summarization:** Identifying and extracting the most important sentences from the original text.\\n*   **Abstractive Summarization:** Generating entirely new, shorter sentences that capture the essence of the original text (more advanced, often uses deep learning models).\\n\\n**Example (Extractive Summarization using `nltk` - requires `nltk.download(\\'punkt\\')` and `nltk.download(\\'stopwords\\')`):**\\n\\n```python\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nfrom collections import defaultdict\\nimport heapq\\n\\ndef summarize_text(text, num_sentences=3):\\n    if not text:\\n        return \"\"\\n\\n    # Tokenize sentences\\n    sentences = sent_tokenize(text)\\n    if len(sentences) <= num_sentences:\\n        return text # Don\\'t summarize if already short enough\\n\\n    # Tokenize words and remove stop words\\n    stop_words = set(stopwords.words(\\'english\\'))\\n    words = word_tokenize(text.lower())\\n    \\n    # Calculate word frequency\\n    word_frequencies = defaultdict(int)\\n    for word in words:\\n        if word.isalnum() and word not in stop_words:\\n            word_frequencies[word] += 1\\n            \\n    # Calculate maximum frequency for normalization\\n    maximum_frequency = max(word_frequencies.values()) if word_frequencies else 1\\n    \\n    # Normalize frequencies\\n    for word in word_frequencies:\\n        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\\n        \\n    # Calculate sentence scores\\n    sentence_scores = defaultdict(int)\\n    for i, sentence in enumerate(sentences):\\n        for word in word_tokenize(sentence.lower()):\\n            if word in word_frequencies:\\n                sentence_scores[i] += word_frequencies[word]\\n    \\n    # Get the top N sentences based on score\\n    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\\n    \\n    # Reconstruct the summary in original sentence order\\n    final_summary = [sentences[j] for j in sorted(summary_sentences)]\\n    return \" \".join(final_summary)\\n\\n# Example\\nlong_article = \"\"\"\\nArtificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem-solving\".\\n\\nAI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), automated decision-making, and competing at the highest level in strategic game systems (such as chess and Go). As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from the definition of AI, having become a routine technology.\\n\"\"\"\\n\\nprint(f\"Original Article (first 200 chars): \\'{long_article[:200]}...\\'\")\\nsummary = summarize_text(long_article, num_sentences=2)\\nprint(f\"Summary (2 sentences): \\'{summary}\\'\")\\n```\\n\\n---\\n\\n**Which \"Trimmer\" do you need?**\\n\\nTo help you best, could you clarify what specific kind of \"trimming\" you\\'re looking to implement?\\n*   Are you trying to clean up user input?\\n*   Are you limiting message length for display?\\n*   Are you trying to remove specific recurring patterns?\\n*   Or are you aiming for more advanced text condensation?\\n\\nLet me know, and we can dive deeper into the specific implementation!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--14c0d21b-e79b-483a-a7f0-7fe94492bb2d-0', usage_metadata={'input_tokens': 43, 'output_tokens': 3585, 'total_tokens': 3628, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1234}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\":\"Chat1\"}}\n",
    "with_session_history.invoke(add_messages,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cffa463",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_messages = HumanMessage(content=\"Can you Access links , if in case i provide you my Github repo link >? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdd5bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_messages.append(new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5b089c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hey Ishn!\\n\\nYes, I remember your name is Ishn, because you just told me again in this conversation! Just to reiterate what I mentioned before, as an AI, I don\\'t have personal memory or a persistent identity for users across different conversations. So, while I remember it *within our current chat session*, I wouldn\\'t retain it for a new, separate conversation unless you tell me again.\\n\\n---\\n\\nRegarding your second question: **Can I access links, like a GitHub repo link?**\\n\\n**No, I cannot directly \"access\" or browse external links or websites.** My capabilities are limited to processing the text that you provide directly to me within our conversation.\\n\\nThis means:\\n*   I cannot click on a GitHub link and navigate through your repository, read your code files, or understand the project structure on my own.\\n*   I don\\'t have real-time internet browsing capabilities.\\n\\n**However, if you copy-paste specific content from your GitHub repo (e.g., code snippets, a `README.md` file, specific error messages, or a description of your project) into our chat, I can certainly read and process that text to help you.**\\n\\nSo, feel free to share relevant code or text directly, and I\\'ll do my best to assist you!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--6d29b1e0-7056-45d6-84cc-65d127272f1d-0', usage_metadata={'input_tokens': 2457, 'output_tokens': 581, 'total_tokens': 3038, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 309}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_session_history.invoke(add_messages,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74bdc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input_texual_message = input(\"Input the Question?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0578c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_message = HumanMessage(content=user_input_texual_message)\n",
    "add_messages.append(user_message)\n",
    "response_final = with_session_history.invoke(add_messages,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85dcc606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hey Ishn!\n",
      "\n",
      "Let's go through your questions:\n",
      "\n",
      "1.  **Do you remember my name?**\n",
      "    Yes, I remember your name is Ishn because you just told me again in this conversation! As I've mentioned, I retain information within our current chat session, but I don't have persistent memory across different conversations. So, for this chat, yes, I know your name is Ishn.\n",
      "\n",
      "2.  **Can you access links, if in case I provide you my GitHub repo link?**\n",
      "    No, I cannot directly \"access\" or browse external links like a GitHub repository. My capabilities are limited to processing the text that you provide directly to me within our conversation. If you copy-paste specific code snippets, a `README.md` file, or other text content from your repo, I can certainly read and help you with that.\n",
      "\n",
      "3.  **What is Agentic AI?**\n",
      "\n",
      "Agentic AI refers to a paradigm where an Artificial Intelligence system is designed to act as an **autonomous agent** capable of **reasoning, planning, executing actions, observing the results, and self-correcting** to achieve a complex goal.\n",
      "\n",
      "Think of it less like a simple question-answering system and more like a skilled project manager or a sophisticated autonomous robot.\n",
      "\n",
      "Here are the key characteristics and components that define Agentic AI:\n",
      "\n",
      "*   **Goal-Oriented:** An agent is given a high-level objective (e.g., \"build a website that does X,\" \"research Y topic and summarize it,\" \"debug this code\").\n",
      "*   **Planning:** It breaks down the high-level goal into a series of smaller, manageable sub-tasks. This often involves an internal \"thought\" process where the AI decides the next logical step.\n",
      "*   **Execution (Tool Use):** It uses various tools to perform these sub-tasks. These tools can be:\n",
      "    *   **Code Interpreters:** To write, execute, and debug code.\n",
      "    *   **Web Browsers/Search Engines:** To gather information from the internet.\n",
      "    *   **APIs:** To interact with external services (e.g., sending emails, updating databases).\n",
      "    *   **File System Access:** To read/write files.\n",
      "*   **Observation/Perception:** After executing an action, it observes the outcome or receives feedback from the environment (e.g., \"the code ran successfully,\" \"the search results are X,\" \"an error occurred\").\n",
      "*   **Reflection/Self-Correction:** Based on the observation, it evaluates whether the action was successful and if it moved closer to the goal. If not, it reflects on what went wrong, adjusts its plan, and tries a different approach. This iterative process of plan-execute-observe-reflect is central.\n",
      "*   **Memory/Context:** It maintains a working memory or context of its past actions, observations, and plans to inform future decisions.\n",
      "\n",
      "**How is it different from a regular LLM (Large Language Model)?**\n",
      "\n",
      "A standard LLM is primarily **reactive**. You give it a prompt, and it generates a response based on its training data. It doesn't inherently plan, execute external tools, or self-correct based on real-world feedback.\n",
      "\n",
      "An Agentic AI, however, **uses an LLM as its \"brain\" or \"reasoning engine.\"** The LLM is the core component that does the planning, reflection, and natural language understanding/generation. But the agent wraps the LLM with the ability to interact with the environment through tools and iterate towards a goal.\n",
      "\n",
      "**Why is Agentic AI important?**\n",
      "\n",
      "*   **Solves Complex Problems:** It can tackle multi-step, dynamic problems that require more than a single, static response.\n",
      "*   **Increased Autonomy:** Reduces the need for constant human intervention.\n",
      "*   **Robustness:** Can recover from errors and adapt to unexpected situations.\n",
      "*   **Leverages Tools:** Greatly expands the capabilities of AI beyond just text generation.\n",
      "\n",
      "**Examples of Agentic AI in action:**\n",
      "\n",
      "*   **Autonomous Code Generation and Debugging:** An agent might write a piece of code, run it, see an error, debug the error, and retry until the code works as intended.\n",
      "*   **Automated Research:** Given a research topic, an agent could use web search, synthesize information, and present a summary.\n",
      "*   **Data Analysis:** An agent could be asked to \"analyze this dataset and find trends,\" leading it to write code, generate visualizations, and interpret results.\n",
      "*   **Personal Assistants:** More advanced assistants that can manage complex schedules, book travel, or handle various tasks by interacting with different online services.\n",
      "\n",
      "In essence, Agentic AI moves beyond just generating text to performing actual, multi-step tasks in the real or digital world.\n"
     ]
    }
   ],
   "source": [
    "response_final.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0bdc024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7b87397",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(\n",
    "    top_k_results=2, doc_content_chars_max=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08e9df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_search = ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "question_user_arxiv = \"Attention is All you need\"\n",
    "user_question = HumanMessage(content=question_user_arxiv)\n",
    "response_arxiv_query = arxiv_search.invoke(question_user_arxiv)\n",
    "response_ai_arxiv = AIMessage(content=response_arxiv_query)\n",
    "\n",
    "new_question = HumanMessage(content=\"What Am i trying to Learn , and What was my name\")\n",
    "Arxiv_final_messages = response_ai_arxiv + user_question + new_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14bd8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_messages.append(Arxiv_final_messages)\n",
    "message_to_pass = add_messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410ff6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = with_session_history.invoke(new_question,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4821133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hey Ishn!\n",
      "\n",
      "You are trying to learn **how to implement a Trimmer in Messages and Chat context**.\n",
      "\n",
      "And your name is **Ishn**.\n"
     ]
    }
   ],
   "source": [
    "new_message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba0863",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000026DF23B8DD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connection.py:174\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     conn = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kw\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\util\\connection.py:95\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m socket.error(\u001b[33m\"\u001b[39m\u001b[33mgetaddrinfo returns an empty list\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\util\\connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNewConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connectionpool.py:716\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m httplib_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[32m    729\u001b[39m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connectionpool.py:416\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m         \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connection.py:244\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers)\u001b[39m\n\u001b[32m    243\u001b[39m     headers[\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m] = _get_default_user_agent()\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python311\\Lib\\http\\client.py:1303\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1302\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1303\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python311\\Lib\\http\\client.py:1349\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1348\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python311\\Lib\\http\\client.py:1298\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python311\\Lib\\http\\client.py:1058\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1061\u001b[39m \n\u001b[32m   1062\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python311\\Lib\\http\\client.py:996\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connection.py:205\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     conn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_conn(conn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connection.py:186\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[32m    187\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % e\n\u001b[32m    188\u001b[39m     )\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[31mNewConnectionError\u001b[39m: <urllib3.connection.HTTPConnection object at 0x0000026DF23B8DD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connectionpool.py:802\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    800\u001b[39m     e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, e)\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\util\\retry.py:594\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_retry.is_exhausted():\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[32m    596\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000026DF23B8DD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n\u001b[32m      2\u001b[39m llm_ollama = ChatOllama(model=\u001b[33m\"\u001b[39m\u001b[33mllama3.2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mllm_ollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is agentic Ai ? \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:383\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    378\u001b[39m     **kwargs: Any,\n\u001b[32m    379\u001b[39m ) -> BaseMessage:\n\u001b[32m    380\u001b[39m     config = ensure_config(config)\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    382\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    393\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:1006\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    999\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1003\u001b[39m     **kwargs: Any,\n\u001b[32m   1004\u001b[39m ) -> LLMResult:\n\u001b[32m   1005\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:825\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    824\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    831\u001b[39m         )\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    833\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:1072\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1076\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\chat_models\\ollama.py:291\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    269\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    272\u001b[39m     **kwargs: Any,\n\u001b[32m    273\u001b[39m ) -> ChatResult:\n\u001b[32m    274\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[32m    275\u001b[39m \n\u001b[32m    276\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    288\u001b[39m \u001b[33;03m            ])\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    299\u001b[39m         message=AIMessage(content=final_chunk.text),\n\u001b[32m    300\u001b[39m         generation_info=final_chunk.generation_info,\n\u001b[32m    301\u001b[39m     )\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations=[chat_generation])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\chat_models\\ollama.py:222\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    214\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    215\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m     **kwargs: Any,\n\u001b[32m    220\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    221\u001b[39m     final_chunk: Optional[ChatGenerationChunk] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\chat_models\\ollama.py:194\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_chat_stream\u001b[39m(\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    186\u001b[39m     messages: List[BaseMessage],\n\u001b[32m    187\u001b[39m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    188\u001b[39m     **kwargs: Any,\n\u001b[32m    189\u001b[39m ) -> Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    190\u001b[39m     payload = {\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._convert_messages_to_ollama_messages(messages),\n\u001b[32m    193\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\llms\\ollama.py:252\u001b[39m, in \u001b[36m_OllamaCommon._create_stream\u001b[39m\u001b[34m(self, api_url, payload, stop, **kwargs)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    247\u001b[39m     request_payload = {\n\u001b[32m    248\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: payload.get(\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    249\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m: payload.get(\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m, []),\n\u001b[32m    250\u001b[39m         **params,\n\u001b[32m    251\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m response.encoding = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\adapters.py:700\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    697\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    703\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000026DF23B8DD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm_ollama = ChatOllama(model=\"llama3.2\")\n",
    "llm_ollama.invoke(\"What is agentic Ai ? \")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
