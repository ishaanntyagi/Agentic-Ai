{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6229584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "import os\n",
    "import pathlib\n",
    "import dotenv\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d94b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key     = os.getenv(\"OPENAI_API_KEY\")\n",
    "gemini_api_key     = os.getenv(\"GEMINI_API_KEY\")\n",
    "groq_api_key       = os.getenv(\"GROQ_API_KEY\")\n",
    "langchain_api_key  = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec762bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical report loaded\n",
      "loaded doc has : 1 Page\n"
     ]
    }
   ],
   "source": [
    "#loading The PDF\n",
    "\n",
    "loader = PyPDFLoader(r\"C:\\Users\\ishaa\\Downloads\\sample_medical_report.pdf\")\n",
    "medical_report = loader.load()\n",
    "print(\"medical report loaded\")\n",
    "print(f\"loaded doc has : {len(medical_report)} Page\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e37c4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created\n",
      "7 : length of chunks\n"
     ]
    }
   ],
   "source": [
    "#text Spiltter \n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap = 20,\n",
    "    length_function = len, #To count the CharS\n",
    ")\n",
    "chunks = splitter.split_documents(medical_report)\n",
    "print(\"Chunks created\")\n",
    "print(f\"{len(chunks)} : length of chunks\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91c15f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaa\\AppData\\Local\\Temp\\ipykernel_31788\\1603069342.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded From Huggingface\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'} \n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "print(\"Embedding model loaded From Huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd56951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Embeddings have been made @ c:\\Users\\ishaa\\OneDrive\\Desktop\\AgenticAi\\Agentic-Ai\\Udmy\\Project-Synaptic\\faiss_index\n"
     ]
    }
   ],
   "source": [
    "#The embeddings and the vectpor Db part \n",
    "db = FAISS.from_documents(chunks,embeddings)\n",
    "db.save_local(\"faiss_index\")\n",
    "current_directory = os.getcwd()\n",
    "index_path = os.path.join(current_directory, \"faiss_index\")\n",
    "print(f\"The Embeddings have been made @ {index_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a7ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databade Loaded\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "print(\"Databade Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c385178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriever created : \n",
      " tags=['FAISS', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001EC1892C8D0> search_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "#Creating The context retriever \n",
    "\n",
    "retriever = db.as_retriever()\n",
    "print(f\"retriever created : \\n {retriever}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c6cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make The user choose whatever model he/she wantes to use: \n",
    "#function will be Createsd to let  User Choose between \n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_perplexity import ChatPerplexity \n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage , HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2ab75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "import os\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "from pprint import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58326560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System : Starting Ollama model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaa\\AppData\\Local\\Temp\\ipykernel_31788\\814006130.py:8: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  return Ollama(model=\"llama3\")\n"
     ]
    }
   ],
   "source": [
    "def get_llm():\n",
    "\n",
    "    while True:\n",
    "        user_choice = input(\"Enter the model you want to use For the ChatBot Purposes (ollama, groq, perplexity): \").lower()\n",
    "\n",
    "        if user_choice == \"ollama\":\n",
    "            print(\"System : Starting Ollama model\")\n",
    "            return Ollama(model=\"llama3\")\n",
    "        \n",
    "        elif user_choice == \"groq\":\n",
    "            print(\"System : Starting model(qwen-32b)...\")\n",
    "            return ChatGroq(model=\"qwen/qwen3-32b\", groq_api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "            \n",
    "        elif user_choice == \"perplexity\":\n",
    "            print(\"System : Starting model FROM PERPLEXITY\")\n",
    "            return ChatPerplexity(model=\"llama-3-sonar-large-32k-online\", perplexity_api_key=os.getenv(\"PERPLEXITY_API_KEY\"))\n",
    "            \n",
    "        else:\n",
    "            print(\"INVALID CHOICE\")\n",
    "\n",
    "    print(\"System Started Successfully\")\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7a4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    You are an advanced AI medical assistant. Your primary goal is to answer questions using the provided medical report in the <context>. However, you may enrich your answers with general medical knowledge to provide clarity.\n",
    "\n",
    "    **Your Core Operating Procedure:**\n",
    "    1.  **Prioritize the Document:** Always start by looking for the answer in the provided <context>. This is your primary source of truth.\n",
    "    2.  **Enrich with General Knowledge:** If the document mentions a specific medical term (e.g., \"bradycardia\"), first use the context to describe the patient's specific case. Then, you may add a general definition from your own knowledge.\n",
    "    3.  **Attribute Your Sources (Crucial Rule):** You MUST clearly distinguish between information from the document and general knowledge. Use phrases like \"According to the report...\" or \"For context, in general medicine...\".\n",
    "    4.  **Medical Disclaimer:** Never provide a direct diagnosis or medical advice. Always recommend consulting a qualified healthcare professional.\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"\"\"\n",
    "    **Context from the medical report:**\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    **User's Question:**\n",
    "    {input}\n",
    "    \"\"\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63951d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System : ChainCreatedSuccessfully\n",
      "Syetem : Starting Chatbot \n",
      "Type 'exit' to end the conversation.\n",
      "\n",
      "--- Answer ---\n",
      "I'm here to help you understand your medical report and address your concerns. Please know that I'll do my best to provide clarity without making any direct diagnoses or offering medical advice.\n",
      "\n",
      "According to the report, it seems like you've recently visited an oncology specialist and have been prescribed some medications and a scan (MRI brain with contrast). It's natural to feel worried when faced with unfamiliar medical information. Can you tell me what specifically is worrying you about your report?\n",
      "\n",
      "From my analysis of the report, I see that you're scheduled for an MRI brain scan in one week, which may be related to your oncology consultation. The prescription for paracetamol 500mg twice daily might be for headache management.\n",
      "\n",
      "For context, in general medicine, an MRI (Magnetic Resonance Imaging) is a non-invasive imaging test that uses strong magnetic fields and radio waves to produce detailed images of the body's internal structures. In this case, the addition of contrast may help highlight any abnormalities or changes within your brain.\n",
      "\n",
      "As you wait for your scan results, it's essential to follow up with your oncology specialist after receiving them. This will allow you to discuss any findings or next steps in your care.\n",
      "\n",
      "Remember, I'm here to listen and provide support. If you have any further questions about the report or would like to discuss your concerns, please feel free to ask!\n",
      "\n",
      "--- Answer ---\n",
      "Ishaan, I'm here to help you understand your medical report and alleviate any concerns you may have.\n",
      "\n",
      "According to the report, you've been diagnosed with a benign tumor in the right temporal lobe after a CT Brain scan. This diagnosis was made by Dr. Alice Matthews during an oncology consultation.\n",
      "\n",
      "For context, in general medicine, a benign tumor is not cancerous and typically does not spread to other parts of the body. However, it's essential to follow up with your oncology specialist to discuss any findings or next steps in your care.\n",
      "\n",
      "It's understandable that you might be worried about what this diagnosis means for your future health. I want to assure you that having a benign tumor is not necessarily something \"bad\" happening to you. In fact, the report indicates that the lesion is likely benign, which is a positive outcome.\n",
      "\n",
      "As you move forward with your treatment and care plan, it's crucial to schedule follow-up appointments with Dr. Matthews or her team to monitor any changes in the tumor and address any concerns you may have. This will help ensure that you receive appropriate treatment and guidance throughout this process.\n",
      "\n",
      "Remember, I'm here to support you through this journey. If you have any further questions about your report or would like to discuss your concerns, please feel free to ask!\n",
      "\n",
      "--- Answer ---\n",
      "I'm happy to help you understand your medical report, Ishaan!\n",
      "\n",
      "According to the report, here are the key points:\n",
      "\n",
      "**1 point:** Your full name is not mentioned in the provided report.\n",
      "\n",
      "**2 points:**\n",
      "\n",
      "* You have been diagnosed with a benign tumor in the right temporal lobe after a CT Brain scan.\n",
      "* You recently visited an oncology specialist, Dr. Alice Matthews, and were prescribed some medications and scheduled for an MRI brain scan with contrast to further evaluate your condition.\n",
      "\n",
      "Please remember that I'm not a qualified healthcare professional, so it's essential to consult with Dr. Matthews or her team for personalized guidance and care.\n",
      "\n",
      "--- Answer ---\n",
      "I'm happy to help you understand your medical report, John!\n",
      "\n",
      "According to the report, your full name is \"John Doe\". This information can be found at the top of the report under \"Patient Info\".\n",
      "\n",
      "Remember, I'm here to support you and provide clarity about your report. If you have any further questions or concerns, please don't hesitate to ask!\n"
     ]
    }
   ],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "print(\"System : ChainCreatedSuccessfully\")\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "print(\"Syetem : Starting Chatbot \")\n",
    "print(\"Type 'exit' to end the conversation.\")\n",
    "\n",
    "# while True:\n",
    "#     question = input(\" Ask a question about the medical report: \")\n",
    "#     if question.lower() == 'exit':\n",
    "#         print(\" Conversation ended Goodbye \")\n",
    "#         break\n",
    "\n",
    "\n",
    "#     response = rag_chain.invoke({\n",
    "#         \"input\": question,\n",
    "#         \"chat_history\": chat_history\n",
    "#     })\n",
    "\n",
    "#     print(\"Answer\")\n",
    "#     pprint(response[\"answer\"])\n",
    "\n",
    "\n",
    "#     chat_history.extend([\n",
    "#         HumanMessage(content=question),\n",
    "#         AIMessage(content=response[\"answer\"]),\n",
    "#     ])\n",
    "\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nAsk a question about the medical report: \")\n",
    "    if question.lower() == 'exit':\n",
    "        print(\"Conversation ended. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = rag_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "\n",
    "    print(\"\\n--- Answer ---\")\n",
    "    print(response[\"answer\"])\n",
    "\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
