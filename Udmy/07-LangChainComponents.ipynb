{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57e360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002B79E6C2190> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002B79E6C30D0> root_client=<openai.OpenAI object at 0x000002B79D797A50> root_async_client=<openai.AsyncOpenAI object at 0x000002B79E6C4690> model_name='gpt-5' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=\"sk-proj-OdD3Oee***************lMvz**************oK2VIKom2xDkFhvJ73v******************VhZ_T2k6Zj7VAhaoqKdEA\",\n",
    "                 model = \"gpt-5\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641c2d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='It looks like you’re asking about “GenAI” and possibly “Agentic AI” (the second term seems misspelled). Here are quick, clear definitions of both, plus “Generic AI” in case that’s what you meant.\\n\\n- Generative AI (GenAI): AI models that create new content—text, images, audio, video, or code—based on patterns learned from large datasets. Examples: ChatGPT/GPT-4, DALL·E, Stable Diffusion. Typical uses: writing, summarizing, image generation, code completion.\\n\\n- Agentic AI (AI agents): Systems that use models like GenAI to plan and take actions toward a goal over multiple steps. They can call tools/APIs, browse, write files, run code, and adjust based on feedback. Examples: AutoGPT-style agents, tool-using ChatGPT, ReAct/LangGraph agents. Typical uses: automating workflows, research-and-execute tasks, data pipelines.\\n\\n- “Generic AI” (if you meant this): Not a standard technical term. People sometimes use it to mean:\\n  - General-purpose AI models (used for many tasks), or\\n  - Narrow AI (ANI): AI that does specific tasks (translation, recommendation), or\\n  - Confusion with AGI (Artificial General Intelligence): human-level, broad capability across tasks (still not achieved).\\n\\nHow they differ:\\n- GenAI generates content on request.\\n- Agentic AI uses (often) a GenAI model inside a loop to decide what to do next, call tools, and move toward a goal.\\n- “Generic AI” is ambiguous; if you meant AGI, that’s the idea of human-level general intelligence.\\n\\nWhich pair did you mean: Generative vs Agentic AI, or Generative vs Generic/AGI?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 954, 'prompt_tokens': 15, 'total_tokens': 969, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 576, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C3PeSesYzgUbO01RIlTeU4R0rjexH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4af6677a-9c40-466d-8f13-9f8947ae8738-0', usage_metadata={'input_tokens': 15, 'output_tokens': 954, 'total_tokens': 969, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 576}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"WHAT IS GENAI AND AGENRIC AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df17223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e8aa84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee1bab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaa\\AppData\\Local\\Temp\\ipykernel_18580\\3231584512.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(chain.run({}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple implementation of binary search in Python:\n",
      "\n",
      "```python\n",
      "def binary_search(arr, target):\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "\n",
      "    while low <= high:\n",
      "        mid = (low + high) // 2\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "        elif arr[mid] < target:\n",
      "            low = mid + 1\n",
      "        else:\n",
      "            high = mid - 1\n",
      "\n",
      "    return -1\n",
      "\n",
      "# Example usage\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "target = 5\n",
      "result = binary_search(arr, target)\n",
      "\n",
      "if result != -1:\n",
      "    print(f\"Element found at index {result}\")\n",
      "else:\n",
      "    print(\"Element not found\")\n",
      "```\n",
      "\n",
      "This code defines a function `binary_search` that takes a sorted array `arr` and a target value `target` as input. It then performs a binary search on the array to find the index of the target value. If the target value is found, the function returns the index of the target value in the array. If the target value is not found, the function returns -1.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"ACT LIKE AN EXPERIENCED DEVELOPER, AND PROVIDE ME A CODE FOR BINARY SEARCH\")\n",
    "llm = ChatOpenAI(api_key=\"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\",\n",
    "                 model=\"gpt-3.5-turbo\",\n",
    "                temperature=0)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9092c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
