{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86d25eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002F1E967CDD0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002F1E967D750>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_key = \"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\" #key has already been Altered.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\",\n",
    "                 groq_api_key=groq_key)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6779401d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, the user is Ishaan, a student learning LangChain. He probably wants to ask questions about LangChain but didn't specify yet. I should respond in a friendly, encouraging way to invite him to ask specific questions. I'll mention common areas where students might need help with LangChain, like integrating with LLMs, building agents, or using memory. Keep the tone positive and supportive. Maybe offer examples or resources if he needs them. Need to make sure my response is clear and not too technical, but still shows I'm here to help with any level of question he might have.\\n</think>\\n\\nHi Ishaan! Nice to meet you ðŸ‘‹ Welcome to the world of LangChain â€“ it's an awesome framework to learn! I'd love to help you with any questions you have about it.\\n\\nSince you're just starting out, maybe we could:\\n1. Discuss the basics (what LangChain does and why it's useful)\\n2. Work through examples (like building simple chains or agents)\\n3. Talk about integrating with LLMs or other tools\\n\\nWhat specific part of LangChain are you currently exploring? Or would you like suggestions on where to start? I'm here to help with explanations, code examples, or practice exercises!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 257, 'prompt_tokens': 25, 'total_tokens': 282, 'completion_time': 0.656533918, 'prompt_time': 1.055318598, 'queue_time': 0.051285634, 'total_time': 1.711852516}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c8944145-be33-47d1-8844-7593ffacf9fe-0', usage_metadata={'input_tokens': 25, 'output_tokens': 257, 'total_tokens': 282})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hy My Name is ishaan, Im A Student and Learning LangChain These Days\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "113017c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user is asking, \"What is My name?\" Let me look back at the conversation history. The user introduced themselves as ishaan at the start. The previous assistant message confirmed that and asked if they needed help. Now, the user is asking their own name again. Since the name was established in the first message, the answer is straightforward. I just need to state their name clearly. I should make sure to spell it correctly as \"ishaan\" without any extra letters. Also, maybe add a friendly note to confirm if they need help with anything related to LangChain or LangGraph. Keep it simple and helpful.\\n</think>\\n\\nYour name is **ishaan**. ðŸ˜Š  \\nLet me know if you need help with LangChain or LangGraph!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 159, 'prompt_tokens': 64, 'total_tokens': 223, 'completion_time': 0.38033164, 'prompt_time': 0.184251229, 'queue_time': 0.051197747, 'total_time': 0.564582869}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a99ade4d-8555-4c60-b0dd-b8f33957fcf1-0', usage_metadata={'input_tokens': 64, 'output_tokens': 159, 'total_tokens': 223})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "\n",
    "    [\n",
    "        HumanMessage(content=\"Hy My Name is ishaan, am a beginner to langchain and Langgraphs\"),\n",
    "        AIMessage(content=\"You Are ishaan and your a Beginner to Langchain and LangGraph , may i help you with anything?\"),\n",
    "        HumanMessage(content=\"What is My name ? \")\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e812fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_community.llms import Ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "426fe0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_session_history = RunnableWithMessageHistory(model,get_session_history)\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ad515f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_session_history.invoke(\n",
    "    [HumanMessage(content=\"Hy My name is Ishaan , Iam New to langchain and LAngGraph.\")],\n",
    "     config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3d84c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is new to LangChain and LangGraph. They introduced their name as Ishaan. I need to respond in a friendly and helpful manner. Let me start by welcoming them and offering assistance. I should explain what LangChain and LangGraph are in simple terms. Maybe mention that LangChain is for building apps with LLMs and LangGraph is for creating workflows. Ask them what specific questions they have or what they\\'re trying to build. Keep the tone encouraging and open-ended. Make sure to invite them to ask more if they need. Avoid technical jargon as much as possible. Check for any typos in the response. Alright, that should cover the basics and prompt them to provide more details.\\n</think>\\n\\nHello Ishaan! Welcome to the world of **LangChain** and **LangGraph** ðŸ˜Š\\n\\n**LangChain** is a powerful framework that helps developers build applications using **Large Language Models (LLMs)** like me (Qwen). It provides tools for tasks like:\\n- **Prompt management** (designing questions for LLMs).\\n- **Memory systems** (storing conversations or data).\\n- **Chains** (sequential workflows).\\n- **Agents** (autonomous workflows for complex tasks).\\n- **Vector stores** (for working with embeddings and semantic search).\\n\\n**LangGraph**, on the other hand, is a **workflow orchestration tool** built on top of LangChain. It lets you design **graph-based workflows** (like flowcharts) to combine tools, LLMs, and custom functions into structured pipelines. For example, you could create a workflow where:\\n1. An LLM analyzes text.\\n2. A tool fetches data from an API.\\n3. Another LLM summarizes the results.\\n\\n---\\n\\n### What can you build with them?\\n- **Q&A systems** (e.g., chatbots with memory).\\n- **Code generation tools** (with error handling via agents).\\n- **Data analysis pipelines** (using LLMs for insights).\\n- **Automated workflows** (e.g., \"Analyze a document â†’ extract key points â†’ generate a report\").\\n\\n---\\n\\n### How can I help you?\\n1. **Explain concepts** (e.g., \"What is a LangChain Agent?\").\\n2. **Debug code** (share your code, and Iâ€™ll help fix it).\\n3. **Guided tutorials** (step-by-step for beginners).\\n4. **Project ideas** (e.g., \"I want to build a chatbotâ€”how to start?\").\\n\\nLet me know what you\\'re curious about or what you\\'re trying to build! ðŸš€'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Changing the Config So that i can Provide new Chat Session id \n",
    "#here i gave the session  iod as chat 1 But in the nextI will be giving as the New ChatWhere the Ai will not be able to Remember My name \n",
    "config2 = {\"configurable\":{\"session_id\": \"chat1\"}} #The Session id was same as previous \n",
    "respone2= with_session_history.invoke(\n",
    "    [HumanMessage(content=\"What is My name? \")],\n",
    "    config = config2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af72aed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user asked, \"What is My name?\" Let me check the conversation history.\\n\\nLooking back, the user introduced themselves as Ishaan when they first started the conversation. The previous response from the assistant acknowledged that name. Now, the user is asking for their name again. \\n\\nHmm, maybe they want to confirm if the assistant remembers their name. Since the assistant already used their name in the initial response, it\\'s possible the user is testing if the memory is retained. \\n\\nBut wait, in the current setup, does the assistant have a memory system? The user is new to LangChain and LangGraph, so maybe they\\'re learning about memory systems. The assistant should answer based on the conversation history provided. \\n\\nIn this case, the history shows the user\\'s name is Ishaan. The assistant should respond with that. No need to overcomplicate; just provide the name again and offer further help. \\n\\nAlso, maybe the user is trying to understand how memory works in LangChain. The assistant can mention that it\\'s using the conversation history to recall the name. That way, it answers the question and educates the user on a relevant concept.\\n</think>\\n\\nYour name is **Ishaan**! ðŸ˜Š  \\n\\nSince you mentioned it earlier in our conversation, I remembered it using the chat history. If you\\'d like to dive deeper into how memory works in **LangChain** (like storing and retrieving data across interactions), feel free to ask! ðŸš€'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respone2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4ca154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Changing the Config So that i can Provide new Chat Session id \n",
    "config3 = {\"configurable\":{\"session_id\": \"chat2\"}}\n",
    "respone3= with_session_history.invoke(\n",
    "    [HumanMessage(content=\"What is My name? \")],\n",
    "    config = config3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee62abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "respone3.content\n",
    "## Lets Apply the StrOutPutParser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "ChatOutput = StrOutputParser()\n",
    "newoutput = ChatOutput.invoke(respone2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d817cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is asking, \"What is My name?\" Let me think about how to handle this. First, I need to remember that I don\\'t have access to personal information unless the user provides it themselves. They might be testing if I can recognize them from previous interactions, but in this case, I don\\'t have a history with them. \\n\\nHmm, maybe they want to set their name in the conversation. But according to the guidelines, I shouldn\\'t store or recall personal information unless it\\'s provided in the current session. Wait, the user might have mentioned their name earlier, but since this is the first message, there\\'s no prior context. \\n\\nI should respond by letting them know I can\\'t access that information and ask them to provide their name if they want me to use it. Also, remind them that I don\\'t retain personal data beyond the current conversation. Need to make sure the response is helpful and clear without overstepping. \\n\\nLet me check again: privacy policies say not to store personal info. So, I can\\'t look up their name from past interactions. The best approach is to politely inform them that I can\\'t know their name unless they tell me now. Yeah, that\\'s the right way to go.\\n</think>\\n\\nI don\\'t have access to personal information about users, including names, unless you provide it directly in our conversation. However, I don\\'t store or retain any personal data between sessions. If you\\'d like, you can tell me your name for the purposes of this conversation, and I\\'ll use it if it helps our interaction!'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newoutput\n",
    "#The output parser Output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82759c42",
   "metadata": {},
   "source": [
    "### Via ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a4b64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported template format: variable_name='Message1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate , MessagesPlaceholder\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m prompt = \u001b[43mChatPromptTemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAct Like you are a Experienced Software Developer and You have to Work at the Best Of Your Ability\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMessagesPlaceholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMessage1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\prompts\\chat.py:1170\u001b[39m, in \u001b[36mChatPromptTemplate.from_messages\u001b[39m\u001b[34m(cls, messages, template_format)\u001b[39m\n\u001b[32m   1129\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_messages\u001b[39m(\n\u001b[32m   1131\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   1132\u001b[39m     messages: Sequence[MessageLikeRepresentation],\n\u001b[32m   1133\u001b[39m     template_format: PromptTemplateFormat = \u001b[33m\"\u001b[39m\u001b[33mf-string\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1134\u001b[39m ) -> ChatPromptTemplate:\n\u001b[32m   1135\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[32m   1136\u001b[39m \n\u001b[32m   1137\u001b[39m \u001b[33;03m    Examples:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1168\u001b[39m \n\u001b[32m   1169\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\prompts\\chat.py:951\u001b[39m, in \u001b[36mChatPromptTemplate.__init__\u001b[39m\u001b[34m(self, messages, template_format, **kwargs)\u001b[39m\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    898\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    899\u001b[39m     messages: Sequence[MessageLikeRepresentation],\n\u001b[32m   (...)\u001b[39m\u001b[32m    902\u001b[39m     **kwargs: Any,\n\u001b[32m    903\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    904\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[32m    905\u001b[39m \n\u001b[32m    906\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m \n\u001b[32m    950\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m     messages_ = \u001b[43m[\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_convert_to_message_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    956\u001b[39m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[32m    957\u001b[39m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\prompts\\chat.py:952\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    898\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    899\u001b[39m     messages: Sequence[MessageLikeRepresentation],\n\u001b[32m   (...)\u001b[39m\u001b[32m    902\u001b[39m     **kwargs: Any,\n\u001b[32m    903\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    904\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[32m    905\u001b[39m \n\u001b[32m    906\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m \n\u001b[32m    950\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    951\u001b[39m     messages_ = [\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m         \u001b[43m_convert_to_message_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    954\u001b[39m     ]\n\u001b[32m    956\u001b[39m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[32m    957\u001b[39m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\prompts\\chat.py:1430\u001b[39m, in \u001b[36m_convert_to_message_template\u001b[39m\u001b[34m(message, template_format)\u001b[39m\n\u001b[32m   1428\u001b[39m     message_ = message\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m     message_ = \u001b[43m_create_template_from_message_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate_format\u001b[49m\n\u001b[32m   1432\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[32m   1434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\prompts\\chat.py:1346\u001b[39m, in \u001b[36m_create_template_from_message_type\u001b[39m\u001b[34m(message_type, template, template_format)\u001b[39m\n\u001b[32m   1332\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a message prompt template from a message type and template string.\u001b[39;00m\n\u001b[32m   1333\u001b[39m \n\u001b[32m   1334\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1343\u001b[39m \u001b[33;03m    ValueError: If unexpected message type.\u001b[39;00m\n\u001b[32m   1344\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m-> \u001b[39m\u001b[32m1346\u001b[39m     message: BaseMessagePromptTemplate = \u001b[43mHumanMessagePromptTemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate_format\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mai\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m   1350\u001b[39m     message = AIMessagePromptTemplate.from_template(\n\u001b[32m   1351\u001b[39m         cast(\u001b[33m\"\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m\"\u001b[39m, template), template_format=template_format\n\u001b[32m   1352\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\prompts\\chat.py:437\u001b[39m, in \u001b[36m_StringImageMessagePromptTemplate.from_template\u001b[39m\u001b[34m(cls, template, template_format, partial_variables, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a class from a string template.\u001b[39;00m\n\u001b[32m    421\u001b[39m \n\u001b[32m    422\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m \u001b[33;03m    ValueError: If the template is not a string or list of strings.\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(template, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     prompt: Union[StringPromptTemplate, \u001b[38;5;28mlist\u001b[39m] = \u001b[43mPromptTemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartial_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartial_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(prompt=prompt, **kwargs)\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(template, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\prompts\\prompt.py:290\u001b[39m, in \u001b[36mPromptTemplate.from_template\u001b[39m\u001b[34m(cls, template, template_format, partial_variables, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_template\u001b[39m(\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m     **kwargs: Any,\n\u001b[32m    259\u001b[39m ) -> PromptTemplate:\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a prompt template from a template.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m    262\u001b[39m \u001b[33;03m    *Security warning*:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    288\u001b[39m \u001b[33;03m        The prompt template loaded from the template.\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     input_variables = \u001b[43mget_template_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m     partial_variables_ = partial_variables \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m partial_variables_:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\prompts\\string.py:261\u001b[39m, in \u001b[36mget_template_variables\u001b[39m\u001b[34m(template, template_format)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    260\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported template format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(input_variables)\n",
      "\u001b[31mValueError\u001b[39m: Unsupported template format: variable_name='Message1'"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Amnswer all the question to the nest of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
