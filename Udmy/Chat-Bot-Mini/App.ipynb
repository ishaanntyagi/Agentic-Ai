{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d25eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000022E3C4C9710>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000022E3C5F0D50>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_key = \"\" #key has already been Altered.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\",\n",
    "                 groq_api_key=groq_key)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6779401d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user is a student named Ishaan who\\'s learning LangChain. He probably wants to understand what LangChain is, how to use it, and maybe some examples to help with his learning. He might be looking for resources or guidance on getting started.\\n\\nFirst, I should explain what LangChain is. It\\'s a framework for developing applications with large language models. It helps manage prompts, call models, handle memory, and integrate with other tools. That\\'s the basic idea.\\n\\nThen, I need to outline the main components. The documentation mentions modules like Chains, Prompts, Memory, Agents, and Callbacks. Each of these plays a role in building applications. For example, Chains are sequences of operations, Prompts are templates for generating input, Memory allows the app to remember past interactions, Agents can make decisions, and Callbacks provide logs or metrics.\\n\\nNext, maybe provide a simple example. A basic chain that takes input, processes it with a prompt, and returns a result. I\\'ll need to use Python code here. Also, mention the steps to get started: install LangChain, set up an API key for a model like OpenAI, and then create a chain.\\n\\nI should also suggest some resources. The official LangChain documentation is essential. Maybe recommend tutorials or courses on platforms like Udemy or Coursera if he wants structured learning. Communities like GitHub or Reddit could help with troubleshooting.\\n\\nWait, does he have any specific questions about LangChain? Maybe he\\'s struggling with a particular part, like setting up the environment or understanding how agents work. But since he\\'s just starting, keeping it general is better. Encourage him to ask more specific questions as he goes.\\n\\nCheck if there are any common mistakes beginners make. For example, not properly setting up the API keys or misunderstanding how to structure a chain. Including a note about API key setup would be helpful.\\n\\nAlso, mention that LangChain supports different models, not just OpenAI. Maybe he\\'s using Hugging Face or another provider, so flexibility is a plus.\\n\\nFinally, wrap it up with an encouraging message. Learning LangChain can be challenging but rewarding. Offer to help with any specific issues he encounters.\\n</think>\\n\\nHi Ishaan! ðŸ˜Š Welcome to the exciting world of **LangChain**! It\\'s great to hear you\\'re diving into this powerful framework for building applications with LLMs (Large Language Models). Iâ€™m here to help you learnâ€”just ask anything, and Iâ€™ll guide you step by step. Letâ€™s break it down:\\n\\n---\\n\\n### **What is LangChain?**  \\nLangChain is a **framework** for developing applications powered by LLMs. It helps you:  \\n- **Chain together** LLMs, prompts, and data.  \\n- **Manage memory** (e.g., chat history).  \\n- **Integrate with external tools** (e.g., APIs, databases).  \\n- **Build agents** that can make decisions.  \\n\\n---\\n\\n### **Key Concepts to Learn**  \\n1. **Chains**  \\n   - Sequences of operations (e.g., a prompt + LLM + post-processing).  \\n   - Example:  \\n     ```python\\n     from langchain.chains import LLMChain\\n     from langchain.prompts import PromptTemplate\\n     from langchain_community.llms import OpenAI\\n\\n     llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\\n     prompt = PromptTemplate.from_template(\"Write a haiku about {topic}.\")\\n     chain = LLMChain(llm=llm, prompt=prompt)\\n     result = chain.run(\"nature\")\\n     print(result)\\n     ```\\n\\n2. **Agents**  \\n   - LLM-powered \"agents\" that can use tools (e.g., search the web or calculate math).  \\n   - Example:  \\n     ```python\\n     from langchain.agents import load_tools, initialize_agent, AgentType\\n\\n     tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\\n     agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\\n     agent.run(\"What is the population of India?\")\\n     ```\\n\\n3. **Memory**  \\n   - Store context between interactions (e.g., chatbots).  \\n   - Example:  \\n     ```python\\n     from langchain.memory import ConversationBufferMemory\\n\\n     memory = ConversationBufferMemory()\\n     chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\\n     chain.run(\"Hello, my name is Ishaan.\")\\n     ```\\n\\n4. **Callbacks**  \\n   - Track execution (e.g., logs, performance metrics).  \\n\\n---\\n\\n### **Get Started**  \\n1. **Install LangChain**:  \\n   ```bash\\n   pip install langchain\\n   ```\\n\\n2. **Set Up an LLM**  \\n   - Use OpenAI (requires an API key):  \\n     ```bash\\n     export OPENAI_API_KEY=\"your-api-key\"\\n     ```\\n   - Or try Hugging Face, Anthropic, or local models.\\n\\n3. **Experiment with Examples**  \\n   - Try the [LangChain tutorials](https://python.langchain.com/v0.2/docs/how_to/).  \\n   - Build a **chatbot**, **document Q&A system**, or **automated agent**.\\n\\n---\\n\\n### **Resources for You**  \\n- **Official Docs**: [https://python.langchain.com/v0.2/](https://python.langchain.com/v0.2/)  \\n- **YouTube Tutorials**: Search for \"LangChain tutorial for beginners\" (e.g., by [LangChain team](https://www.youtube.com/@langchains)).  \\n- **Courses**: Platforms like Udemy or Coursera have LangChain-focused courses.  \\n- **Community**: Join the [LangChain Discord](https://discord.gg/7qJGJvDy96) for help!  \\n\\n---\\n\\n### **Want to Try Something?**  \\nAsk me:  \\n- \"How do I build a chatbot with memory?\"  \\n- \"How can I use LangChain with Hugging Face models?\"  \\n- \"Can you explain agents in simple terms?\"  \\n\\nHappy learning, Ishaan! ðŸš€ Youâ€™re on the right path. If you hit a roadblock, just askâ€”letâ€™s solve it together!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1314, 'prompt_tokens': 25, 'total_tokens': 1339, 'completion_time': 4.357163244, 'prompt_time': 0.010183049, 'queue_time': 0.044260971, 'total_time': 4.367346293}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ebf78529-d2bd-4332-b02a-e90d6008c846-0', usage_metadata={'input_tokens': 25, 'output_tokens': 1314, 'total_tokens': 1339})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hy My Name is ishaan, Im A Student and Learning LangChain These Days\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "113017c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user is asking, \"What is My name?\" Let me check the conversation history. The user introduced themselves as Ishaan at the beginning. The previous interaction was about being a beginner in LangChain and LangGraph. The user probably wants confirmation of their name.\\n\\nI should make sure there\\'s no confusion. The name was clearly stated as Ishaan. Maybe they want to test if I remember correctly or if there was a typo. I need to respond affirmatively to confirm their name. Also, since they mentioned being a beginner, maybe offer help related to their interests. Let me structure the response to be friendly and open for assistance.\\n</think>\\n\\nYour name is **Ishaan**! ðŸ˜Š  \\nHow can I assist you today with **LangChain** or **LangGraphs**? Let me know if you\\'re stuck on a specific concept, or if you\\'d like guidance on getting started. Happy to help! ðŸš€', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 196, 'prompt_tokens': 64, 'total_tokens': 260, 'completion_time': 0.54560956, 'prompt_time': 0.012895334, 'queue_time': 0.044725823, 'total_time': 0.558504894}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--85b7b865-08de-41f9-a4d0-4ae572cfa748-0', usage_metadata={'input_tokens': 64, 'output_tokens': 196, 'total_tokens': 260})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "\n",
    "    [\n",
    "        HumanMessage(content=\"Hy My Name is ishaan, am a beginner to langchain and Langgraphs\"),\n",
    "        AIMessage(content=\"You Are ishaan and your a Beginner to Langchain and LangGraph , may i help you with anything?\"),\n",
    "        HumanMessage(content=\"What is My name ? \")\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e812fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_community.llms import Ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "426fe0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_session_history = RunnableWithMessageHistory(model,get_session_history)\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad515f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_session_history.invoke(\n",
    "    [HumanMessage(content=\"Hy My name is Ishaan , Iam New to langchain and LAngGraph.\")],\n",
    "     config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d84c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is Ishaan and is new to LangChain and LangGraph. I need to respond in a friendly and encouraging way. Let me first welcome him and then explain the two concepts in simple terms. LangChain is a framework for building applications with large language models, while LangGraph is a library for creating directed acyclic graphs (DAGs) of stateful, asynchronous computations. I should mention their purposes and maybe suggest some starting points for learning, like tutorials or documentation. Also, offer help if he has specific questions. Keep it clear and not too technical since he\\'s new.\\n</think>\\n\\nHello Ishaan! ðŸŒŸ  \\nWelcome to the world of **LangChain** and **LangGraph**! I\\'m excited to help you get started.  \\n\\n### Quick Overview:  \\n1. **LangChain**  \\n   - A framework for building applications powered by **Large Language Models (LLMs)**.  \\n   - Helps you combine LLMs with tools, memory, agents, and more (e.g., chatbots, data analysis, automation).  \\n\\n2. **LangGraph**  \\n   - A **stateful, asynchronous workflow** library for Python.  \\n   - Focuses on creating **Directed Acyclic Graphs (DAGs)** of tasks, often used to orchestrate complex workflows (e.g., data pipelines, multi-step processes).  \\n\\n### How They Work Together:  \\nLangChain can integrate with LangGraph to create **LLM-powered workflows**. For example:  \\n- Use LangGraph to design a workflow where an LLM (via LangChain) generates a query, then fetches data, processes it, and returns a result.  \\n\\n---\\n\\n### Suggested Learning Path:  \\n1. **Start with LangChain**:  \\n   - [Official LangChain Docs](https://python.langchain.com/) (great for tutorials on LLMs, agents, memory, etc.).  \\n   - Try the **LangChain Expression Language (LCEL)** for simple chains.  \\n\\n2. **Explore LangGraph** (once youâ€™re comfortable with LangChain):  \\n   - [LangGraph GitHub](https://github.com/langchain-ai/langgraph) (check the `examples/` folder).  \\n   - Learn how to define nodes and edges in a graph for workflows.  \\n\\n3. **Combine Both** (advanced):  \\n   - Use LangChainâ€™s `Runnable` interface with LangGraph to create workflows like:  \\n     ```python\\n     from langgraph.graph import Graph\\n     from langchain_core.runnables import RunnableLambda\\n\\n     def step1(input): return f\"LLM processed: {input}\"\\n     def step2(input): return f\"Final result: {input}\"\\n\\n     workflow = Graph()\\n     workflow.add_node(\"step1\", RunnableLambda(step1))\\n     workflow.add_node(\"step2\", RunnableLambda(step2))\\n     workflow.add_edge(\"step1\", \"step2\")\\n     workflow.set_entry_point(\"step1\")\\n     workflow.set_finish_point(\"step2\")\\n\\n     result = workflow.invoke(\"Hello, LangChain!\")\\n     print(result)  # Output: \"Final result: LLM processed: Hello, LangChain!\"\\n     ```\\n\\n---\\n\\n### Ask Me Anything!  \\nIf you have specific questions or want to build a project (e.g., a chatbot, workflow, or data pipeline), just let me know. Iâ€™m here to help! ðŸ˜Š  \\n\\nHappy coding! ðŸš€'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91a5fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Changing the Config So that i can Provide new Chat Session id \n",
    "#here i gave the session  iod as chat 1 But in the nextI will be giving as the New ChatWhere the Ai will not be able to Remember My name \n",
    "config2 = {\"configurable\":{\"session_id\": \"chat1\"}} #The Session id was same as previous \n",
    "respone2= with_session_history.invoke(\n",
    "    [HumanMessage(content=\"What is My name? \")],\n",
    "    config = config2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af72aed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is asking, \"What is My name?\" Let me check the conversation history.\\n\\nLooking back, the user introduced themselves as Ishaan in the first message. The previous response from the assistant also addressed the user as Ishaan. Since the user is now asking for their name again, it\\'s a simple retrieval question. The name hasn\\'t been changed since the start of the conversation, so the answer should be straightforward. Just need to confirm there\\'s no conflicting information. All right, the answer is Ishaan.\\n</think>\\n\\nYour name is **Ishaan**! ðŸ˜Š  \\nYou mentioned it in your first message. Let me know if there\\'s anything else I can help with!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respone2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4ca154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Changing the Config So that i can Provide new Chat Session id \n",
    "config3 = {\"configurable\":{\"session_id\": \"chat2\"}}\n",
    "respone3= with_session_history.invoke(\n",
    "    [HumanMessage(content=\"What is My name? \")],\n",
    "    config = config3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee62abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "respone3.content\n",
    "## Lets Apply the StrOutPutParser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "ChatOutput = StrOutputParser()\n",
    "newoutput = ChatOutput.invoke(respone2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d817cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is asking, \"What is My name?\" Let me check the conversation history.\\n\\nLooking back, the user introduced themselves as Ishaan in the first message. The previous response from the assistant also addressed the user as Ishaan. Since the user is now asking for their name again, it\\'s a simple retrieval question. The name hasn\\'t been changed since the start of the conversation, so the answer should be straightforward. Just need to confirm there\\'s no conflicting information. All right, the answer is Ishaan.\\n</think>\\n\\nYour name is **Ishaan**! ðŸ˜Š  \\nYou mentioned it in your first message. Let me know if there\\'s anything else I can help with!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newoutput\n",
    "#The output parser Output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82759c42",
   "metadata": {},
   "source": [
    "### Via ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b1a4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Amnswer all the question to the nest of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea771dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user is asking, \"What is my name?\" I need to figure out how to respond appropriately. First, I should remember that I don\\'t have access to personal information unless the user provides it. So in this case, the user is asking for their own name, which I don\\'t know unless they\\'ve told me before.\\n\\nLet me check the conversation history. Since we\\'re starting fresh, there\\'s no previous context where the user mentioned their name. That means I can\\'t infer it from past messages. So my response should be to ask them to provide their name. \\n\\nI need to make sure my response is polite and helpful. Maybe say something like, \"I don\\'t have access to your personal information. Could you please tell me your name?\" That way, I\\'m acknowledging that I can\\'t retrieve it on my own and inviting them to share it if they want to.\\n\\nI should also consider if there\\'s any other way the name could be determined, but since there\\'s no prior interaction, there\\'s no data to pull from. So the best course of action is to ask them directly. No need to overcomplicate it. Just a straightforward request for their name.\\n</think>\\n\\nI donâ€™t have access to your personal information, including your name. If youâ€™d like to share it with me, feel free to tell me! ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 278, 'prompt_tokens': 36, 'total_tokens': 314, 'completion_time': 0.68371522, 'prompt_time': 0.007210039, 'queue_time': 0.051195717, 'total_time': 0.690925259}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f802a382-ac60-4e56-b40f-cbe8329486d4-0', usage_metadata={'input_tokens': 36, 'output_tokens': 278, 'total_tokens': 314})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"What is my name ? \")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c54c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_session_history = RunnableWithMessageHistory(chain,get_session_history)\n",
    "config_new = {\"configurable\": {\"session_id\": \"Chat3\"}}\n",
    "response_Chat = with_session_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name ? Do you remember ? Also What was Your Name ? \")],\n",
    "    config = config_new\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1048e9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is Ishaan and is new to LangChain and LangGraph. I need to respond in a friendly and encouraging way. Let me first welcome him and then explain the two concepts in simple terms. LangChain is a framework for building applications with large language models, while LangGraph is a library for creating directed acyclic graphs (DAGs) of stateful, asynchronous computations. I should mention their purposes and maybe suggest some starting points for learning, like tutorials or documentation. Also, offer help if he has specific questions. Keep it clear and not too technical since he\\'s new.\\n</think>\\n\\nHello Ishaan! ðŸŒŸ  \\nWelcome to the world of **LangChain** and **LangGraph**! I\\'m excited to help you get started.  \\n\\n### Quick Overview:  \\n1. **LangChain**  \\n   - A framework for building applications powered by **Large Language Models (LLMs)**.  \\n   - Helps you combine LLMs with tools, memory, agents, and more (e.g., chatbots, data analysis, automation).  \\n\\n2. **LangGraph**  \\n   - A **stateful, asynchronous workflow** library for Python.  \\n   - Focuses on creating **Directed Acyclic Graphs (DAGs)** of tasks, often used to orchestrate complex workflows (e.g., data pipelines, multi-step processes).  \\n\\n### How They Work Together:  \\nLangChain can integrate with LangGraph to create **LLM-powered workflows**. For example:  \\n- Use LangGraph to design a workflow where an LLM (via LangChain) generates a query, then fetches data, processes it, and returns a result.  \\n\\n---\\n\\n### Suggested Learning Path:  \\n1. **Start with LangChain**:  \\n   - [Official LangChain Docs](https://python.langchain.com/) (great for tutorials on LLMs, agents, memory, etc.).  \\n   - Try the **LangChain Expression Language (LCEL)** for simple chains.  \\n\\n2. **Explore LangGraph** (once youâ€™re comfortable with LangChain):  \\n   - [LangGraph GitHub](https://github.com/langchain-ai/langgraph) (check the `examples/` folder).  \\n   - Learn how to define nodes and edges in a graph for workflows.  \\n\\n3. **Combine Both** (advanced):  \\n   - Use LangChainâ€™s `Runnable` interface with LangGraph to create workflows like:  \\n     ```python\\n     from langgraph.graph import Graph\\n     from langchain_core.runnables import RunnableLambda\\n\\n     def step1(input): return f\"LLM processed: {input}\"\\n     def step2(input): return f\"Final result: {input}\"\\n\\n     workflow = Graph()\\n     workflow.add_node(\"step1\", RunnableLambda(step1))\\n     workflow.add_node(\"step2\", RunnableLambda(step2))\\n     workflow.add_edge(\"step1\", \"step2\")\\n     workflow.set_entry_point(\"step1\")\\n     workflow.set_finish_point(\"step2\")\\n\\n     result = workflow.invoke(\"Hello, LangChain!\")\\n     print(result)  # Output: \"Final result: LLM processed: Hello, LangChain!\"\\n     ```\\n\\n---\\n\\n### Ask Me Anything!  \\nIf you have specific questions or want to build a project (e.g., a chatbot, workflow, or data pipeline), just let me know. Iâ€™m here to help! ðŸ˜Š  \\n\\nHappy coding! ðŸš€'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

